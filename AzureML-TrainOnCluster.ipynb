{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pickle\nwith open(\"test.txt\", \"wb\") as file:\n    pickle.dump(str(\"Hello\"), file)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport azureml\nfrom azureml.core import Workspace, Run\n\n# check core SDK version number\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "subscription_id = '6a0ec27b-1111-1111-1111-8c3003d5e4bc'\nresource_group  = 'PythonSDK'\nworkspace_name  = 'pythonsdkworkspace'\n\ntry:\n    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n    ws.write_config()\n    print('Library configuration succeeded')\nexcept:\n    print('Workspace not found')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "experiment_name = 'sklearn-mnist'\n\nfrom azureml.core import Experiment\nexp = Experiment(workspace=ws, name=experiment_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ws.compute_targets",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\n# choose a name for your cluster\nAmlCompute_cluster_name = \"AmlComputeGPU\"\ncluster_min_nodes = 0\ncluster_max_nodes = 4\nvm_size = \"STANDARD_NC6\"\nautoscale_enabled = True\n\n\nif AmlCompute_cluster_name in ws.compute_targets:\n    compute_target = ws.compute_targets[AmlCompute_cluster_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + AmlCompute_cluster_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size, # NC6 is GPU-enabled\n                                                                vm_priority = 'lowpriority', # optional Low Priority VMs use Azure's excess capacity and are thus cheaper but risk your run being pre-empted.\n                                                                min_nodes = cluster_min_nodes, \n                                                                max_nodes = cluster_max_nodes,\n                                                                idle_seconds_before_scaledown=120)\n    \n    # create the cluster\n    compute_target = ComputeTarget.create(ws, AmlCompute_cluster_name, provisioning_config)\n    \n    # can poll for a minimum number of nodes and for a specific timeout. \n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n    \n     # For a more detailed view of current BatchAI cluster status, use the 'status' property    \n    print(compute_target.status.serialize())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Download the MNIST dataset (handwritten digit database)\nDownload the MNIST dataset and save the files into a data directory locally. Images and labels for both training and testing are downloaded."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport urllib.request\n\nos.makedirs('./dataMNIST', exist_ok = True)\n\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', filename='./dataMNIST/train-images.gz')\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', filename='./dataMNIST/train-labels.gz')\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename='./dataMNIST/test-images.gz')\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename='./dataMNIST/test-labels.gz')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Display some sample images"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# make sure utils.py is in the same directory as this code\nfrom utils import load_data\n\n# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the model converge faster.\nX_train = load_data('./dataMNIST/train-images.gz', False) / 255.0\ny_train = load_data('./dataMNIST/train-labels.gz', True).reshape(-1)\n\nX_test = load_data('./dataMNIST/test-images.gz', False) / 255.0\ny_test = load_data('./dataMNIST/test-labels.gz', True).reshape(-1)\n\n# now let's show some randomly chosen images from the traininng set.\ncount = 0\nsample_size = 30\nplt.figure(figsize = (16, 6))\nfor i in np.random.permutation(X_train.shape[0])[:sample_size]:\n    count = count + 1\n    plt.subplot(1, sample_size, count)\n    plt.axhline('')\n    plt.axvline('')\n    plt.text(x=10, y=-10, s=y_train[i], fontsize=18)\n    plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.Greys)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Upload tothe cloud\nds = ws.get_default_datastore()\nprint(ds.datastore_type, ds.account_name, ds.container_name)\n\nds.upload(src_dir='./dataMNIST', target_path='mnist', overwrite=True, show_progress=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Train in the notebook"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%time\n#Train a local model - this will take a while 5min 28s\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_hat = clf.predict(X_test)\nprint(np.average(y_hat == y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Train on a remote cluster\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Create a directory for the scripts\nimport os\nscript_folder = './sklearn-mnist'\nos.makedirs(script_folder, exist_ok=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Create a training script"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $script_folder/train.py\n\nimport argparse\nimport os\nimport numpy as np\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\n\nfrom azureml.core import Run\nfrom utils import load_data\n\n# let user feed in 2 parameters, the location of the data files (from datastore), and the regularization rate of the logistic regression model\nparser = argparse.ArgumentParser()\nparser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\nparser.add_argument('--regularization', type=float, dest='reg', default=0.01, help='regularization rate')\nargs = parser.parse_args()\n\ndata_folder = os.path.join(args.data_folder, 'mnist')\nprint('Data folder:', data_folder)\n\n# load train and test set into numpy arrays\n# note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\nX_train = load_data(os.path.join(data_folder, 'train-images.gz'), False) / 255.0\nX_test = load_data(os.path.join(data_folder, 'test-images.gz'), False) / 255.0\ny_train = load_data(os.path.join(data_folder, 'train-labels.gz'), True).reshape(-1)\ny_test = load_data(os.path.join(data_folder, 'test-labels.gz'), True).reshape(-1)\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep = '\\n')\n\n# get hold of the current run\nrun = Run.get_context()\n\nprint('Train a logistic regression model with regularizaion rate of', args.reg)\nclf = LogisticRegression(C=1.0/args.reg, random_state=42)\nclf.fit(X_train, y_train)\n\nprint('Predict the test set')\ny_hat = clf.predict(X_test)\n\n# calculate accuracy on the prediction\nacc = np.average(y_hat == y_test)\nprint('Accuracy is', acc)\n\nrun.log('regularization rate', np.float(args.reg))\nrun.log('accuracy', np.float(acc))\n\nos.makedirs('outputs', exist_ok=True)\n# note file saved in the outputs folder is automatically uploaded into experiment record\njoblib.dump(value=clf, filename='outputs/sklearn_mnist_model.pkl')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Copy across utils python packege as well\nimport shutil\nshutil.copy('utils.py', script_folder)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#An estimator object is used to submit the run. \n#Define the data folder and the learning rate\nfrom azureml.train.estimator import Estimator\n\nscript_params = {\n    '--data-folder': ds.as_mount(),\n    '--regularization': 0.8\n}\n\nest = Estimator(source_directory=script_folder,\n                script_params=script_params,\n                compute_target=compute_target,\n                use_gpu=True,\n                entry_script='train.py',\n                conda_packages=['scikit-learn'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Run the job on the cluster\nrun = exp.submit(config=est)\nrun",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Monitor the job in the cluster\nfrom azureml.widgets import RunDetails\nRunDetails(run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Hyerparameter Training"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Train Hyperparameters\nfrom azureml.train.hyperdrive import *\n\nps = RandomParameterSampling(\n    {\n        '--regularization': uniform(0.001, 0.9)\n    }\n)\n\n\n#from azureml.train.hyperdrive import RandomParameterSampling\n#param_sampling = RandomParameterSampling( {\n#        \"--regularization\": normal(10, 3)\n#    }\n#)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "htc = HyperDriveRunConfig(estimator=est, \n                          hyperparameter_sampling=ps, \n                          primary_metric_name='accuracy', \n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                          max_total_runs=4,\n                          max_concurrent_runs=4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "htr = exp.submit(config=htc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "RunDetails(htr).show()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}